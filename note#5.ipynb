{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜µí‹°ë§ˆì´ì €"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer í´ë˜ìŠ¤: ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ì„ ìœ„í•œ ê¸°ë°˜ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ì„ ìœ„í•œ ê¸°ë°˜ í´ë˜ìŠ¤\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "\n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "\n",
    "    def update_one(self, param):        # êµ¬ì²´ì ì¸ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
    "        raise NotImplementedError()     # ì•„ë‹´ ë“±ì˜ ì˜µí‹°ë§ˆì´ì € ì—¬ê¸°ì— êµ¬í˜„\n",
    "\n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD í´ë˜ìŠ¤ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ë¬¸ì œ í’€ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•œ ìµœì í™” ê¸°ë²•<br>\n",
    ": Momentum, AdaGrad, AdaDelta, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) WëŠ” ê°±ì‹ í•  ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ \n",
    "2) ğœ•ğ¿/ğœ•ğ‘Šì€ ê¸°ìš¸ê¸°\n",
    "3) Î· ëŠ” í•™ìŠµë¥ \n",
    "4) vëŠ” ì†ë„\n",
    "5) Î±v ëŠ” ë¬¼ì²´ê°€ ì•„ë¬´ëŸ° í˜ì„ ë°›ì§€ ì•Šì„ë•Œ ì„œì„œíˆ ê°ì†ì‹œí‚¤ëŠ” ì—­í• "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MomentumSGD êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            xp = cuda.get_array_module(param.data)\n",
    "            self.vs[v_key] = xp.zeros_like(param.data)\n",
    "\n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ë‹¤ì¤‘ í´ë˜ìŠ¤(Multi-class Classification) ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": ì—¬ëŸ¬ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì‹ ê²½ë§ìœ¼ë¡œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ í˜• íšŒê·€ ë•Œ ì´ìš©í•œ ì‹ ê²½ë§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "\n",
    "model = MLP((10, 3))\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": ì‹ ê²½ë§ ì¶œë ¥ì´ ìˆ˜ì¹˜ì¸ë°, ì´ ìˆ˜ì¹˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜, ì´í•©ì´ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)    \n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë°°ì¹˜(batch) ë°ì´í„°ì—ë„ ì†Œí”„íŠ¸ë§¥ìˆ˜ í•¨ìˆ˜ ì ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ êµ¬í˜„<br>\n",
    ": ì •ë‹µì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ë©´ 1ë¡œ, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ê¸°ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)  # x_min ì´í•˜ë©´ x_min ìœ¼ë¡œ ë³€í™˜í•˜ê³ , x_max ì´ìƒì´ë©´ x_maxë¡œ ë³€í™˜\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": ìŠ¤íŒŒì´ëŸ´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì‹¤ì œ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "#ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # ì¸ë±ìŠ¤ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter): #  ë¯¸ë‹ˆë°°ì¹˜ ìƒì„±\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "        # ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ í•¨ìˆ˜ ê²°ê³¼ ì¶œë ¥\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": ê±°ëŒ€í•œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ndarray ì¸ìŠ¤í„´ìŠ¤ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´ì „ìš© í´ë˜ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset í´ë˜ìŠ¤ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset: # ê¸°ë°˜ í´ë˜ìŠ¤\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\\n",
    "                   self.target_transform(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì–´í…ì…˜(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ì¡´ì˜ seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoderì—ì„œ ë§ˆì§€ë§‰ timestepì˜ hidden stateë§Œì„ Decoderì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤.\n",
    "* ê¸°ë³¸ì ì¸ ëª¨ë¸ì˜ ê²½ìš°, ì¸ì½”ë”ì˜ ì¶œë ¥ì´ ê³ ì • ê¸¸ì´ ë²¡í„°<br>\n",
    "-> ì—„ì²­ë‚œ ê¸¸ì´ì˜ ë¬¸ì¥ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ\n",
    "í•„ìš”í•œ ì •ë³´ê°€ ë²¡í„°ì— ë‹¤ ë‹´ê¸°ì§€ ëª»í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°œì„ ëœ seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê° timestep ë§ˆë‹¤ hidden stateì˜ í–‰ë ¬ì¸ hs ì „ë¶€ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ Decoderë¥¼ ê°œì„ í•œë‹¤.\n",
    "* ì•ì„œ ê°œì„ ëœ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–»ì€ íˆë“  ìŠ¤í…Œì´íŠ¸ë¥¼ ì´ìš©í•˜ì—¬\n",
    "ë””ì½”ë” ê³¼ì •ì—ì„œ ì–´ë–¤ ë‹¨ì–´ë“¤ë¼ë¦¬ ì„œë¡œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ ê·¸ ëŒ€ì‘ê´€ê³„ë¥¼ ëª¨ë¸ì— í•™ìŠµì‹œí‚¨ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> ë”°ë¼ì„œ, í•„ìš”í•œ ì •ë³´ì—ë§Œ ì£¼ëª©í•˜ì—¬ ê·¸ ì •ë³´ë¡œë¶€í„° ì‹œê³„ì—´ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ Decoderì˜ ëª©í‘œì´ë©°, ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ Attentionì´ë¼ í•œë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì–´í…ì…˜ êµ¬ì¡°\n",
    ": ê°€ì¤‘ì¹˜ ê³„ì‚° ê³„ì¸µ, ì„ íƒì‘ì—…ê³„ì¸µ, ê²°í•© ê³„ì¸µ ì´ 3ê°€ì§€ê°€ ìˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°€ì¤‘ì¹˜ ê³„ì‚°ì¸µì—ì„œëŠ” ê° ë‹¨ì–´ì— ëŒ€í•´ì„œ ê·¸ê²ƒì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” â€˜ê°€ì¤‘ì¹˜ aâ€™ë¥¼ êµ¬í•œë‹¤.\n",
    "\n",
    "ì„ íƒì‘ì—…ê³„ì¸µì—ì„œëŠ” ì¸ì½”ë”ê°€ ë‚´ë±‰ì€ íˆë“  ìŠ¤í…Œì´íŠ¸ì™€ ê°€ì¤‘ì¹˜ ê³„ì‚°ì¸µì—ì„œ êµ¬í•œ ê°€ì¤‘ì¹˜ë¥¼ ë”í•œë‹¤ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê°€ì¤‘ì¹˜ ê³„ì‚°ì¸µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []    #(1)\n",
    "        self.cache = None       #(2)\n",
    "        \n",
    "    def forward(self, hs, a):   \n",
    "        N, T, H = hs.shape  \n",
    "        \n",
    "        ar= a.reshape(N, T, 1)  #(4) \n",
    "        t = hs * ar             #(5)\n",
    "        c = np.sum(t, axis=1)   #(6)\n",
    "        \n",
    "        self.cache = (hs, ar)   #(7)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache                         # (8)\n",
    "        N, T, H = hs.shape                          # (9)\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # (10)\n",
    "        dar = dt * hs                               # (11)\n",
    "        dhs = dt * ar                               # (12)\n",
    "        da = np.sum(dar, axis=2)                    # (13)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ë§¤ê°œë³€ìˆ˜(params)ì™€ ë¯¸ë¶„ê°’(grads)ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸<br>\n",
    "(2) ìˆœì „íŒŒ(forward) ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ê°’ì„ ì €ì¥í•˜ì—¬ ì—­ì „íŒŒ(backward) ë‹¨ê³„ì—ì„œ ì‚¬ìš©<br>\n",
    "(3) ìˆœì „íŒŒ. hsëŠ” hidden state, Nì€ batch size, <br>TëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´, HëŠ” ì€ë‹‰ì¸µ ìˆ˜, aëŠ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜<br>\n",
    "(4) ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì¬ë°°ì—´í•˜ì—¬ ê° ì‹œí€€ìŠ¤ ìš”ì†Œì— ëŒ€ì‘í•˜ë„ë¡ í•©<br>\n",
    "(5) hidden statesì™€ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©<br>\n",
    "(6) ì°¨ì›ì— ëŒ€í•˜ì—¬ í•©ì‚°í•˜ì—¬ ê°€ì¤‘í•©ì„ ê³„ì‚°<br>\n",
    "(7) ì—­ì „íŒŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ê°’ì„ ì €ì¥\n",
    "\n",
    "(8) ìºì‹œ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°<br>\n",
    "(9) ì¶œë ¥ í˜•ìƒ ì¶”ì¶œ<br>\n",
    "(10) ì¶œë ¥ ë¯¸ë¶„ê°’ ì¬ë°°ì—´<br>\n",
    "(11) ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ë¯¸ë¶„ê°’ ê³„ì‚°<br>\n",
    "(12) íˆë“  ìƒíƒœì— ëŒ€í•œ ë¯¸ë¶„ê°’ ê³„ì‚°<br>\n",
    "(13) ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ê°’ í•©ì‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì„ íƒì‘ì—…ê³„ì¸µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []    #(1)\n",
    "        self.softmax = Softmax()            #(2)\n",
    "        self.cache = None                   #(3)\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape  \n",
    "\n",
    "        hr = h.reshape(N, 1, H)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)               #(4)\n",
    "        a = self.softmax.forward(s)         #(5)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache                 #(6)\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)      #(7)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)            #(8)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ë¶ˆëŸ¬ì˜¬ ê°’ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°. ë§¤ê°œë³€ìˆ˜, ë¯¸ë¶„ê°’, ê°€ì¤‘ì¹˜í•© í•¨ìˆ˜, ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜<br>\n",
    "(2) ê³„ì‚°ëœ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥<br>\n",
    "(3) ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°, ê°€ì¤‘í•© ê³„ì‚°<br>\n",
    "(4) ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì €ì¥<br>\n",
    "(5) ê°€ì¤‘í•© ë°˜í™˜ <br>\n",
    "(6) doutë¥¼ ê°€ì¤‘í•© í•¨ìˆ˜ì— ëŒ€í•´ ì—­ì „íŒŒí•˜ì—¬ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ë¯¸ë¶„ê°’(da)ì™€ dhs0ì„ ê³„ì‚°<br>\n",
    "(7) ë¯¸ë¶„ê°’ daë¥¼ ì—­ì „íŒŒí•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë¯¸ë¶„ê°’(dhs1)ê³¼ í˜„ì¬ ì°¨ì›ì— ëŒ€í•œ ë¯¸ë¶„ê°’(dh)ì„ ê³„ì‚°<br>\n",
    "(8) ë‘ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë¯¸ë¶„ê°’ì„ í•©í•˜ì—¬ ìµœì¢… ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë¯¸ë¶„ê°’ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê°€ì¤‘í•© ì—­ì „íŒŒë¥¼ í†µí•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì™€ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•œ í›„, ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì—­ì „íŒŒë¥¼ í†µí•´ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ í˜„ì¬ íˆë“  ìƒíƒœì— ëŒ€í•œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []                #(1)\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None                    \n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)  #(2)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a                       #(3)\n",
    "        return out                                      \n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout) #(5)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da) #(6)\n",
    "        dhs = dhs0 + dhs1                               #(7)\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ˆê¸°í™” ë©”ì„œë“œ<br>\n",
    "(1) íŒŒë¼ë¯¸í„°ì™€ ë¯¸ë¶„ê°’, ìºì‹œë¥¼ ì´ˆê¸°í™”,<br>\n",
    "ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜, ê°€ì¤‘í•© í•¨ìˆ˜, ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”<br>\n",
    "ìˆœì „íŒŒ ë©”ì„œë“œ<br>\n",
    "(2) ì¸ì½”ë”ì˜ ì€ë‹‰ìƒíƒœ hsì™€ ì¶œë ¥ëœ ì€ë‹‰ìƒíƒœ hë¥¼ ê°€ì§€ê³  ê°€ì¤‘ì¹˜ ê³„ì‚°, <br>ê·¸í›„ì— ê°€ì¤‘í•© ì€ë‹‰ìƒíƒœì™€ ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ ê°€ì¤‘í•©<br>\n",
    "(3) ë‚˜ì˜¨ ê°’ ê°€ì¤‘ì¹˜ëŠ” ì–´í…ì…˜ ì›¨ì´íŠ¸ë¡œ ì €ì¥í•˜ê³  ê°€ì¤‘í•©ì„ ë°˜í™˜<br>\n",
    "ì—­ì „íŒŒ ë©”ì„œë“œ<br>\n",
    "(4) ë¯¸ë¶„ëœ ê°€ì¤‘í•©ì„ ê°€ì¤‘í•© ì—­ì „íŒŒë¥¼ ê±°ì³ ê°€ì¤‘ì¹˜ ë¯¸ë¶„ê°’ê³¼ ë¯¸ë¶„ëœ íˆë“  ìŠ¤í…Œì´íŠ¸ì„ ê³„ì‚°<br>\n",
    "(5)ë¯¸ë¶„ëœ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì¤‘ì¹˜ ê³„ì¸µì˜ ì—­ì „íŒŒë¥¼ ê±°ì³ íˆë“ ìŠ¤í…Œì´íŠ¸ì˜ ë¯¸ë¶„ê°’ dhì™€  íˆë“ ìŠ¤í…Œì´ì¸ ì˜ ì¶”ê°€ì ì¸ ë¯¸ë¶„ê°’ ê³„ì‚°<br>\n",
    "(6) êµ¬í•œ íˆë“ ìŠ¤í…Œì´íŠ¸ì˜ ë¯¸ë¶„ê°’ 0,1ì„ ë”í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ê³„ì‚°ëœ íˆë“ ìŠ¤í…Œì´ì¸ ì˜ ë¯¸ë¶„ê°’ dhsê³¼ í˜„ì¬ íˆë“  ìŠ¤í…Œì´ì¸ ì˜ ë¯¸ë¶„ê°’ dhë¥¼ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ë¡ ì ìœ¼ë¡œ ì´ëŸ° êµ¬í˜„ë“¤ë¡œ í†µí•´ êµ¬í•œ ê°’ë“¤ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ì—ë§Œ ì£¼ëª©í•˜ì—¬ ë”ìš± ì •í™•í•œ ê²°ê³¼ë¥¼ ë‚´ë†“ì„ ìˆ˜ ìˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
