{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer 클래스: 매개변수 갱신을 위한 기반 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:    # 매개변수 갱신을 위한 기반 클래스\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "\n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "\n",
    "    def update_one(self, param):        # 구체적인 매개변수 갱신\n",
    "        raise NotImplementedError()     # 아담 등의 옵티마이저 여기에 구현\n",
    "\n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 클래스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법으로 매개변수를 갱신하는 클래스를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 클래스를 사용하여 회귀 문제 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기울기를 이용한 최적화 기법<br>\n",
    ": Momentum, AdaGrad, AdaDelta, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) W는 갱신할 가중치 매개변수 \n",
    "2) 𝜕𝐿/𝜕𝑊은 기울기\n",
    "3) η 는 학습률\n",
    "4) v는 속도\n",
    "5) αv 는 물체가 아무런 힘을 받지 않을때 서서히 감속시키는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MomentumSGD 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            xp = cuda.get_array_module(param.data)\n",
    "            self.vs[v_key] = xp.zeros_like(param.data)\n",
    "\n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  다중 클래스(Multi-class Classification) 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 여러 클래스로 분류하는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망으로 다중 클래스 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀 때 이용한 신경망을 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "\n",
    "model = MLP((10, 3))\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 소프트맥스 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 신경망 출력이 수치인데, 이 수치를 확률로 변환, 총합이 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)    \n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치(batch) 데이터에도 소프트맥수 함수 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*교차 엔트로피 오차 구현<br>\n",
    ": 정답에 해당하는 클래스면 1로, 그렇지 않으면 0으로 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)  # x_min 이하면 x_min 으로 변환하고, x_max 이상이면 x_max로 변환\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 클래스 분류 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 스파이럴 데이터셋을 사용하여 다중 클래스 분류 실제 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "#모델과 옵티마이저를 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 인덱스를 무작위로 섞음\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter): #  미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "        # 기울기를 구하고 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 에포크마다 손실 함수 결과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대규모 데이터셋 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 거대한 데이터를 하나의 ndarray 인스턴스로 처리하려면전용 클래스 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset: # 기반 클래스\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\\n",
    "                   self.target_transform(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존의 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoder에서 마지막 timestep의 hidden state만을 Decoder의 입력으로 사용했다.\n",
    "* 기본적인 모델의 경우, 인코더의 출력이 고정 길이 벡터<br>\n",
    "-> 엄청난 길이의 문장이 입력되었을 때\n",
    "필요한 정보가 벡터에 다 담기지 못하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선된 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 timestep 마다 hidden state의 행렬인 hs 전부를 활용할 수 있도록 Decoder를 개선한다.\n",
    "* 앞서 개선된 인코더를 사용하여 얻은 히든 스테이트를 이용하여\n",
    "디코더 과정에서 어떤 단어들끼리 서로 연관되어 있는지 그 대응관계를 모델에 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 따라서, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 Decoder의 목표이며, 이러한 구조를 Attention이라 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 구조\n",
    ": 가중치 계산 계층, 선택작업계층, 결합 계층 총 3가지가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 계산층에서는 각 단어에 대해서 그것이 얼마나 중요한지를 나타내는 ‘가중치 a’를 구한다.\n",
    "\n",
    "선택작업계층에서는 인코더가 내뱉은 히든 스테이트와 가중치 계산층에서 구한 가중치를 더한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가중치 계산층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []    #(1)\n",
    "        self.cache = None       #(2)\n",
    "        \n",
    "    def forward(self, hs, a):   \n",
    "        N, T, H = hs.shape  \n",
    "        \n",
    "        ar= a.reshape(N, T, 1)  #(4) \n",
    "        t = hs * ar             #(5)\n",
    "        c = np.sum(t, axis=1)   #(6)\n",
    "        \n",
    "        self.cache = (hs, ar)   #(7)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache                         # (8)\n",
    "        N, T, H = hs.shape                          # (9)\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # (10)\n",
    "        dar = dt * hs                               # (11)\n",
    "        dhs = dt * ar                               # (12)\n",
    "        da = np.sum(dar, axis=2)                    # (13)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 매개변수(params)와 미분값(grads)를 저장하는 리스트<br>\n",
    "(2) 순전파(forward) 단계에서 사용된 값을 저장하여 역전파(backward) 단계에서 사용<br>\n",
    "(3) 순전파. hs는 hidden state, N은 batch size, <br>T는 시퀀스 길이, H는 은닉층 수, a는 어텐션 가중치<br>\n",
    "(4) 어텐션 가중치를 재배열하여 각 시퀀스 요소에 대응하도록 합<br>\n",
    "(5) hidden states와 가중치를 곱하여 가중치를 적용<br>\n",
    "(6) 차원에 대하여 합산하여 가중합을 계산<br>\n",
    "(7) 역전파 단계에서 사용할 값을 저장\n",
    "\n",
    "(8) 캐시 값 불러오기<br>\n",
    "(9) 출력 형상 추출<br>\n",
    "(10) 출력 미분값 재배열<br>\n",
    "(11) 가중치에 대한 미분값 계산<br>\n",
    "(12) 히든 상태에 대한 미분값 계산<br>\n",
    "(13) 어텐션 가중치의 미분값 합산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 선택작업계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []    #(1)\n",
    "        self.softmax = Softmax()            #(2)\n",
    "        self.cache = None                   #(3)\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape  \n",
    "\n",
    "        hr = h.reshape(N, 1, H)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)               #(4)\n",
    "        a = self.softmax.forward(s)         #(5)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache                 #(6)\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)      #(7)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)            #(8)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 불러올 값들 불러오기. 매개변수, 미분값, 가중치합 함수, 가중치 계산 함수<br>\n",
    "(2) 계산된 어텐션 가중치를 저장<br>\n",
    "(3) 어텐션 가중치 계산, 가중합 계산<br>\n",
    "(4) 어텐션 가중치 저장<br>\n",
    "(5) 가중합 반환 <br>\n",
    "(6) dout를 가중합 함수에 대해 역전파하여 어텐션 가중치에 대한 미분값(da)와 dhs0을 계산<br>\n",
    "(7) 미분값 da를 역전파하여 입력 시퀀스에 대한 미분값(dhs1)과 현재 차원에 대한 미분값(dh)을 계산<br>\n",
    "(8) 두 단계에서 계산된 입력 시퀀스에 대한 미분값을 합하여 최종 입력 시퀀스에 대한 미분값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중합 역전파를 통해 어텐션 가중치와 입력 시퀀스에 대한 그라디언트를 계산한 후, 어텐션 가중치 역전파를 통해 입력 시퀀스와 현재 히든 상태에 대한 그라디언트를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []                #(1)\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None                    \n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)  #(2)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a                       #(3)\n",
    "        return out                                      \n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout) #(5)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da) #(6)\n",
    "        dhs = dhs0 + dhs1                               #(7)\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화 메서드<br>\n",
    "(1) 파라미터와 미분값, 캐시를 초기화,<br>\n",
    "가중치 계산 함수, 가중합 함수, 어텐션 가중치를 초기화<br>\n",
    "순전파 메서드<br>\n",
    "(2) 인코더의 은닉상태 hs와 출력된 은닉상태 h를 가지고 가중치 계산, <br>그후에 가중합 은닉상태와 가중치를 통해 가중합<br>\n",
    "(3) 나온 값 가중치는 어텐션 웨이트로 저장하고 가중합을 반환<br>\n",
    "역전파 메서드<br>\n",
    "(4) 미분된 가중합을 가중합 역전파를 거쳐 가중치 미분값과 미분된 히든 스테이트을 계산<br>\n",
    "(5)미분된 가중치를 가중치 계층의 역전파를 거쳐 히든스테이트의 미분값 dh와  히든스테이츠의 추가적인 미분값 계산<br>\n",
    "(6) 구한 히든스테이트의 미분값 0,1을 더하여 최종적으로 계산된 히든스테이츠의 미분값 dhs과 현재 히든 스테이츠의 미분값 dh를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론적으로 이런 구현들로 통해 구한 값들을 통해 필요한 정보에만 주목하여 더욱 정확한 결과를 내놓을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
